# ==============================================================================
# MAHT-Net Hyperparameter Tuning - Learning Rate & Optimizer
# ==============================================================================
# Configurations for learning rate and optimizer tuning.
# Each experiment modifies specific parameters from the base config.
# ==============================================================================

# Base config to inherit from
_base_: "maht_net_base.yaml"

# Grid search configurations for LR tuning
experiments:
  # --------------------------------------------------------------------------
  # Experiment 1: Learning Rate Sweep
  # --------------------------------------------------------------------------
  - name: "lr_sweep_1e4"
    description: "Conservative learning rate"
    optimizer:
      lr_backbone: 0.00001
      lr_transformer: 0.0001
      lr_vam: 0.0001
      lr_decoder: 0.0001
      
  - name: "lr_sweep_5e4"
    description: "Moderate learning rate"
    optimizer:
      lr_backbone: 0.00005
      lr_transformer: 0.0005
      lr_vam: 0.0005
      lr_decoder: 0.0005
      
  - name: "lr_sweep_1e3"
    description: "Standard learning rate (recommended)"
    optimizer:
      lr_backbone: 0.0001
      lr_transformer: 0.001
      lr_vam: 0.001
      lr_decoder: 0.001
      
  - name: "lr_sweep_3e3"
    description: "Aggressive learning rate"
    optimizer:
      lr_backbone: 0.0003
      lr_transformer: 0.003
      lr_vam: 0.003
      lr_decoder: 0.003

  # --------------------------------------------------------------------------
  # Experiment 2: Optimizer Comparison
  # --------------------------------------------------------------------------
  - name: "optimizer_adam"
    description: "Standard Adam optimizer"
    optimizer:
      type: "Adam"
      weight_decay: 0.0001
      betas: [0.9, 0.999]
      
  - name: "optimizer_adamw"
    description: "AdamW with decoupled weight decay (recommended)"
    optimizer:
      type: "AdamW"
      weight_decay: 0.01
      betas: [0.9, 0.999]
      
  - name: "optimizer_radam"
    description: "Rectified Adam - variance-aware"
    optimizer:
      type: "RAdam"
      weight_decay: 0.01
      betas: [0.9, 0.999]
      
  - name: "optimizer_sgd"
    description: "SGD with momentum"
    optimizer:
      type: "SGD"
      momentum: 0.9
      weight_decay: 0.0001
      nesterov: true

  # --------------------------------------------------------------------------
  # Experiment 3: Weight Decay Sweep
  # --------------------------------------------------------------------------
  - name: "wd_none"
    optimizer:
      weight_decay: 0.0
      
  - name: "wd_1e4"
    optimizer:
      weight_decay: 0.0001
      
  - name: "wd_1e3"
    optimizer:
      weight_decay: 0.001
      
  - name: "wd_1e2"
    description: "Strong regularization (recommended for AdamW)"
    optimizer:
      weight_decay: 0.01

  # --------------------------------------------------------------------------
  # Experiment 4: Scheduler Comparison
  # --------------------------------------------------------------------------
  - name: "scheduler_cosine"
    description: "Cosine annealing (recommended)"
    scheduler:
      type: "cosine"
      cosine:
        T_0: 10
        T_mult: 2
        eta_min: 0.000001
        
  - name: "scheduler_cosine_warmup"
    description: "Cosine with linear warmup"
    scheduler:
      type: "cosine_warmup"
      warmup_epochs: 5
      warmup_start_lr: 0.0001
      cosine:
        eta_min: 0.000001
        
  - name: "scheduler_step"
    description: "Step decay every 15 epochs"
    scheduler:
      type: "step"
      step:
        step_size: 15
        gamma: 0.1
        
  - name: "scheduler_plateau"
    description: "Reduce on plateau (adaptive)"
    scheduler:
      type: "reduce_on_plateau"
      plateau:
        factor: 0.5
        patience: 5
        min_lr: 0.000001

  # --------------------------------------------------------------------------
  # Experiment 5: Differential Learning Rates
  # --------------------------------------------------------------------------
  - name: "diff_lr_10x"
    description: "Backbone 10x lower than head"
    optimizer:
      lr_backbone: 0.0001
      lr_transformer: 0.001
      lr_vam: 0.001
      lr_decoder: 0.001
      
  - name: "diff_lr_100x"
    description: "Backbone 100x lower than head"
    optimizer:
      lr_backbone: 0.00001
      lr_transformer: 0.001
      lr_vam: 0.001
      lr_decoder: 0.001
      
  - name: "diff_lr_uniform"
    description: "Same LR for all components"
    optimizer:
      lr_backbone: 0.0005
      lr_transformer: 0.0005
      lr_vam: 0.0005
      lr_decoder: 0.0005

  # --------------------------------------------------------------------------
  # Experiment 6: Batch Size vs Learning Rate
  # (Linear scaling rule: double batch_size â†’ double LR)
  # --------------------------------------------------------------------------
  - name: "bs4_lr_half"
    training:
      batch_size: 4
    optimizer:
      lr_backbone: 0.00005
      lr_transformer: 0.0005
      lr_vam: 0.0005
      lr_decoder: 0.0005
      
  - name: "bs8_lr_base"
    training:
      batch_size: 8
    optimizer:
      lr_backbone: 0.0001
      lr_transformer: 0.001
      lr_vam: 0.001
      lr_decoder: 0.001
      
  - name: "bs16_lr_double"
    description: "Requires gradient accumulation on small GPU"
    training:
      batch_size: 8
      accumulation_steps: 2  # Effective batch size = 16
    optimizer:
      lr_backbone: 0.0002
      lr_transformer: 0.002
      lr_vam: 0.002
      lr_decoder: 0.002

  # --------------------------------------------------------------------------
  # Experiment 7: Gradient Clipping
  # --------------------------------------------------------------------------
  - name: "grad_clip_none"
    optimizer:
      gradient_clip: null
      
  - name: "grad_clip_0.5"
    optimizer:
      gradient_clip: 0.5
      
  - name: "grad_clip_1.0"
    description: "Standard gradient clipping (recommended)"
    optimizer:
      gradient_clip: 1.0
      
  - name: "grad_clip_5.0"
    optimizer:
      gradient_clip: 5.0

# ==============================================================================
# Recommended starting point based on architecture document
# ==============================================================================
recommended:
  name: "lr_recommended"
  description: "Recommended LR configuration from architecture spec"
  optimizer:
    type: "AdamW"
    lr_backbone: 0.0001  # Lower for pretrained
    lr_transformer: 0.001
    lr_vam: 0.001
    lr_decoder: 0.001
    weight_decay: 0.01
    gradient_clip: 1.0
  scheduler:
    type: "cosine_warmup"
    warmup_epochs: 5
    warmup_start_lr: 0.0001
    cosine:
      T_0: 10
      T_mult: 2
      eta_min: 0.000001
