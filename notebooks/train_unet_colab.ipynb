{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50583f4c",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a734ebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6846cc02",
   "metadata": {},
   "source": [
    "## 2. Navigate to Project Directory\n",
    "\n",
    "**Update this path to where you uploaded the project in your Google Drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fc240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Update this path!\n",
    "PROJECT_PATH = '/content/drive/MyDrive/spondylolisthesis-maht-net'\n",
    "\n",
    "os.chdir(PROJECT_PATH)\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0430d68e",
   "metadata": {},
   "source": [
    "## 3. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290a34c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8417b5ee",
   "metadata": {},
   "source": [
    "## 4. Verify GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933be86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"⚠️ WARNING: GPU not available. Training will be slow on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc5821d",
   "metadata": {},
   "source": [
    "## 5. Verify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6fcf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Check data paths\n",
    "data_root = Path('data/Train/Keypointrcnn_data')\n",
    "train_images = data_root / 'images' / 'train'\n",
    "train_labels = data_root / 'labels' / 'train'\n",
    "val_images = data_root / 'images' / 'val'\n",
    "val_labels = data_root / 'labels' / 'val'\n",
    "\n",
    "print(\"Data directories:\")\n",
    "print(f\"  Train images: {train_images.exists()} - {len(list(train_images.glob('*')))} files\")\n",
    "print(f\"  Train labels: {train_labels.exists()} - {len(list(train_labels.glob('*.json')))} files\")\n",
    "print(f\"  Val images:   {val_images.exists()} - {len(list(val_images.glob('*')))} files\")\n",
    "print(f\"  Val labels:   {val_labels.exists()} - {len(list(val_labels.glob('*.json')))} files\")\n",
    "\n",
    "if all([train_images.exists(), train_labels.exists(), val_images.exists(), val_labels.exists()]):\n",
    "    print(\"\\n✓ All data directories found!\")\n",
    "else:\n",
    "    print(\"\\n✗ ERROR: Some data directories are missing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20ff272",
   "metadata": {},
   "source": [
    "## 6. Run Quick Tests (Optional but Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b28d3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive tests\n",
    "!python scripts/test_unet.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358a6a2c",
   "metadata": {},
   "source": [
    "## 7. Test Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1e91c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "from src.data.unet_dataset import create_unet_dataloaders\n",
    "from src.data.preprocessing import ImagePreprocessor\n",
    "from src.data.augmentation import SpondylolisthesisAugmentation\n",
    "\n",
    "# Create preprocessor\n",
    "preprocessor = ImagePreprocessor(\n",
    "    target_size=(512, 512),\n",
    "    normalize=True,\n",
    "    apply_clahe=True\n",
    ")\n",
    "\n",
    "# Create augmentation\n",
    "augmentation = SpondylolisthesisAugmentation(mode='train')\n",
    "\n",
    "# Create dataloaders (small batch for testing)\n",
    "print(\"Creating dataloaders...\")\n",
    "train_loader, val_loader = create_unet_dataloaders(\n",
    "    train_image_dir=train_images,\n",
    "    train_label_dir=train_labels,\n",
    "    val_image_dir=val_images,\n",
    "    val_label_dir=val_labels,\n",
    "    batch_size=4,\n",
    "    num_workers=2,\n",
    "    heatmap_sigma=3.0,\n",
    "    output_stride=1,\n",
    "    preprocessor=preprocessor,\n",
    "    augmentation=augmentation\n",
    ")\n",
    "\n",
    "print(f\"✓ Train samples: {len(train_loader.dataset)}\")\n",
    "print(f\"✓ Val samples:   {len(val_loader.dataset)}\")\n",
    "print(f\"✓ Train batches: {len(train_loader)}\")\n",
    "print(f\"✓ Val batches:   {len(val_loader)}\")\n",
    "\n",
    "# Test loading a batch\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"\\n✓ Batch loaded successfully:\")\n",
    "print(f\"  Images shape:   {batch['images'].shape}\")\n",
    "print(f\"  Heatmaps shape: {batch['heatmaps'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d01542",
   "metadata": {},
   "source": [
    "## 8. Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e51e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get a sample\n",
    "sample = train_loader.dataset[0]\n",
    "\n",
    "# Denormalize image for visualization\n",
    "image = sample['image'].permute(1, 2, 0).numpy()\n",
    "image = (image - image.min()) / (image.max() - image.min())\n",
    "\n",
    "# Get heatmaps\n",
    "heatmaps = sample['heatmaps'].numpy()\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Original image\n",
    "axes[0, 0].imshow(image)\n",
    "axes[0, 0].set_title('Original Image')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Heatmaps\n",
    "heatmap_titles = ['Bottom-Left Corner', 'Bottom-Right Corner', \n",
    "                  'Top-Left Corner', 'Top-Right Corner']\n",
    "for i in range(4):\n",
    "    row = (i + 1) // 3\n",
    "    col = (i + 1) % 3\n",
    "    axes[row, col].imshow(heatmaps[i], cmap='hot')\n",
    "    axes[row, col].set_title(heatmap_titles[i])\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "# Combined heatmap\n",
    "combined = np.max(heatmaps, axis=0)\n",
    "axes[1, 2].imshow(image)\n",
    "axes[1, 2].imshow(combined, cmap='hot', alpha=0.5)\n",
    "axes[1, 2].set_title('All Keypoints Overlay')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Sample filename: {sample['filename']}\")\n",
    "print(f\"Number of vertebrae: {len(sample['keypoints'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ce0d50",
   "metadata": {},
   "source": [
    "## 9. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16100ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.unet import create_unet\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model\n",
    "model = create_unet(\n",
    "    in_channels=3,\n",
    "    num_keypoints=4,\n",
    "    bilinear=False,\n",
    "    base_channels=64\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n✓ Model created successfully\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "dummy_input = torch.randn(1, 3, 512, 512).to(device)\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_input)\n",
    "print(f\"  Input shape:  {dummy_input.shape}\")\n",
    "print(f\"  Output shape: {output.shape}\")\n",
    "print(f\"\\n✓ Model ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce0a307",
   "metadata": {},
   "source": [
    "## 10. Start Training\n",
    "\n",
    "### Option A: Use the training script (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb2f115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using the script\n",
    "!python scripts/train_unet.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f55b73",
   "metadata": {},
   "source": [
    "### Option B: Custom training loop in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f448ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch.optim as optim\n",
    "from scripts.train_unet import UNetKeypointLoss, UNetTrainer\n",
    "\n",
    "# Load config\n",
    "with open('experiments/configs/unet_config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Update batch size for Colab (adjust based on GPU memory)\n",
    "config['training']['batch_size'] = 8  # Reduce if OOM\n",
    "config['training']['num_epochs'] = 50\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"  Epochs: {config['training']['num_epochs']}\")\n",
    "print(f\"  Learning rate: {config['training']['learning_rate']}\")\n",
    "\n",
    "# Recreate dataloaders with training batch size\n",
    "train_loader, val_loader = create_unet_dataloaders(\n",
    "    train_image_dir=train_images,\n",
    "    train_label_dir=train_labels,\n",
    "    val_image_dir=val_images,\n",
    "    val_label_dir=val_labels,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    num_workers=2,\n",
    "    heatmap_sigma=3.0,\n",
    "    output_stride=1,\n",
    "    preprocessor=preprocessor,\n",
    "    augmentation=augmentation\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "criterion = UNetKeypointLoss(use_focal=True, focal_alpha=2.0, focal_beta=4.0)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=config['training']['learning_rate'],\n",
    "    weight_decay=config['training']['weight_decay']\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=config['training']['step_size'],\n",
    "    gamma=config['training']['gamma']\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "save_dir = Path('experiments/results/unet_colab')\n",
    "trainer = UNetTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=device,\n",
    "    config=config,\n",
    "    save_dir=save_dir\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Trainer initialized. Starting training...\\n\")\n",
    "\n",
    "# Train\n",
    "trainer.train(config['training']['num_epochs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491c3402",
   "metadata": {},
   "source": [
    "## 11. Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cb6051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(trainer.train_losses, label='Train Loss')\n",
    "plt.plot(trainer.val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(trainer.val_losses, label='Val Loss', color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Validation Loss')\n",
    "plt.axhline(y=trainer.best_val_loss, color='r', linestyle='--', label=f'Best: {trainer.best_val_loss:.4f}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest validation loss: {trainer.best_val_loss:.4f}\")\n",
    "print(f\"Final train loss: {trainer.train_losses[-1]:.4f}\")\n",
    "print(f\"Final val loss: {trainer.val_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901a88b7",
   "metadata": {},
   "source": [
    "## 12. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb28e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(save_dir / 'best_unet_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"✓ Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
    "print(f\"  Validation loss: {checkpoint['val_loss']:.4f}\")\n",
    "\n",
    "# Get a validation sample\n",
    "val_sample = val_loader.dataset[0]\n",
    "image = val_sample['image'].unsqueeze(0).to(device)\n",
    "target_heatmaps = val_sample['heatmaps'].numpy()\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    pred_heatmaps = torch.sigmoid(model(image))\n",
    "pred_heatmaps = pred_heatmaps.cpu().squeeze(0).numpy()\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "\n",
    "# Original image\n",
    "img_np = val_sample['image'].permute(1, 2, 0).numpy()\n",
    "img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
    "\n",
    "for i in range(4):\n",
    "    # Original image\n",
    "    axes[0, i].imshow(img_np)\n",
    "    axes[0, i].set_title(f'Corner {i+1}')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Target heatmap\n",
    "    axes[1, i].imshow(target_heatmaps[i], cmap='hot')\n",
    "    axes[1, i].set_title('Target')\n",
    "    axes[1, i].axis('off')\n",
    "    \n",
    "    # Predicted heatmap\n",
    "    axes[2, i].imshow(pred_heatmaps[i], cmap='hot')\n",
    "    axes[2, i].set_title('Predicted')\n",
    "    axes[2, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Inference successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f404f0",
   "metadata": {},
   "source": [
    "## 13. Download Results\n",
    "\n",
    "Download the trained model and results back to your local machine or keep in Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53b9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results are already saved in Google Drive at:\n",
    "print(f\"Results saved in: {save_dir}\")\n",
    "print(f\"\\nFiles:\")\n",
    "for f in save_dir.glob('*'):\n",
    "    print(f\"  - {f.name}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
