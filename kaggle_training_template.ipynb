{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2a32c54",
   "metadata": {},
   "source": [
    "# Spondylolisthesis Detection - Kaggle Training\n",
    "\n",
    "**Template notebook for training on Kaggle with P100 GPU**\n",
    "\n",
    "## üìã Before Running:\n",
    "\n",
    "1. **Upload Dataset** (One-time setup):\n",
    "   - Go to [kaggle.com/datasets](https://www.kaggle.com/datasets)\n",
    "   - Click \"New Dataset\"\n",
    "   - Upload `spondylolisthesis-dataset.zip`\n",
    "   - Title: `Spondylolisthesis Vertebral Landmark Dataset`\n",
    "   - Set to Private\n",
    "\n",
    "2. **Attach Dataset to This Notebook**:\n",
    "   - Click \"Add Data\" (right panel)\n",
    "   - Search for your dataset: \"Spondylolisthesis Vertebral Landmark Dataset\"\n",
    "   - Click \"Add\"\n",
    "\n",
    "3. **Enable GPU**:\n",
    "   - Settings ‚Üí Accelerator ‚Üí GPU P100\n",
    "   - Click \"Save\"\n",
    "\n",
    "## üöÄ Then Run All Cells Below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43053169",
   "metadata": {},
   "source": [
    "## 1. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4356670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Environment Check\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a51ed11",
   "metadata": {},
   "source": [
    "## 2. Get Latest Code from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6330bccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Always start from /kaggle/working\n",
    "%cd /kaggle/working\n",
    "\n",
    "# Check if repo already exists\n",
    "if os.path.exists('spondylolisthesis-maht-net'):\n",
    "    print(\"Repository exists - pulling latest changes...\")\n",
    "    %cd spondylolisthesis-maht-net\n",
    "    !git pull origin main\n",
    "    print(\"‚úì Code updated successfully\")\n",
    "else:\n",
    "    print(\"Cloning repository for the first time...\")\n",
    "    !git clone https://github.com/mohamednourdine/spondylolisthesis-maht-net.git\n",
    "    %cd spondylolisthesis-maht-net\n",
    "    print(\"‚úì Code cloned successfully\")\n",
    "\n",
    "print(f\"\\nCurrent directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8134721",
   "metadata": {},
   "source": [
    "## 3. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bf839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install project in editable mode (required for imports from src/)\n",
    "# Note: Most dependencies (PyTorch, albumentations, etc.) are pre-installed in Kaggle\n",
    "!pip install -q -e .\n",
    "\n",
    "print(\"‚úì Project installed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2d192c",
   "metadata": {},
   "source": [
    "## 4. Link Dataset\n",
    "\n",
    "**Important**: Make sure you've attached your dataset in the Kaggle UI (Add Data ‚Üí Your Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb09269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# List available datasets\n",
    "print(\"Available datasets in /kaggle/input/:\")\n",
    "print(\"=\"*60)\n",
    "for item in os.listdir('/kaggle/input'):\n",
    "    print(f\"  - {item}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Expected dataset path (adjust if your dataset has a different name)\n",
    "# Kaggle converts spaces to hyphens and makes lowercase\n",
    "dataset_name = 'spondylolisthesis-vertebral-landmark-dataset'\n",
    "dataset_path = f'/kaggle/input/{dataset_name}'\n",
    "\n",
    "# Check if dataset exists\n",
    "if os.path.exists(dataset_path):\n",
    "    print(f\"\\n‚úì Dataset found at: {dataset_path}\")\n",
    "    print(f\"\\nDataset structure:\")\n",
    "    !ls -la {dataset_path}\n",
    "else:\n",
    "    print(f\"\\n‚ùå Dataset not found at: {dataset_path}\")\n",
    "    print(\"\\n‚ö†Ô∏è  Please attach your dataset:\")\n",
    "    print(\"   1. Click 'Add Data' in right panel\")\n",
    "    print(\"   2. Search for 'Spondylolisthesis Vertebral Landmark Dataset'\")\n",
    "    print(\"   3. Click 'Add'\")\n",
    "    print(\"   4. Re-run this cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282d38bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create proper data structure by copying files\n",
    "# This is more reliable than symlinks for this dataset structure\n",
    "\n",
    "dataset_name = 'spondylolisthesis-vertebral-landmark-dataset'\n",
    "dataset_path = f'/kaggle/input/{dataset_name}'\n",
    "\n",
    "# Remove old data directory if exists\n",
    "!rm -rf data\n",
    "\n",
    "# Create data directory with proper structure\n",
    "!mkdir -p data/Train/images data/Train/labels data/Validation/images data/Validation/labels\n",
    "\n",
    "# Copy files instead of symlinking (more reliable)\n",
    "print(\"Copying dataset files...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Copy train images and labels\n",
    "!cp -r {dataset_path}/Train/Keypointrcnn_data/images/train/* data/Train/images/\n",
    "!cp -r {dataset_path}/Train/Keypointrcnn_data/labels/train/* data/Train/labels/\n",
    "\n",
    "# Copy validation images and labels\n",
    "!cp -r {dataset_path}/Train/Keypointrcnn_data/images/val/* data/Validation/images/\n",
    "!cp -r {dataset_path}/Train/Keypointrcnn_data/labels/val/* data/Validation/labels/\n",
    "\n",
    "# Verify final structure\n",
    "print(\"\\nFinal data structure:\")\n",
    "print(\"=\"*60)\n",
    "!ls data/\n",
    "print(\"\\nTrain images (first 5):\")\n",
    "!ls data/Train/images/ | head -5\n",
    "print(f\"\\nTotal train images: {len(os.listdir('data/Train/images/'))}\")\n",
    "print(f\"Total train labels: {len(os.listdir('data/Train/labels/'))}\")\n",
    "print(f\"Total val images: {len(os.listdir('data/Validation/images/'))}\")\n",
    "print(f\"Total val labels: {len(os.listdir('data/Validation/labels/'))}\")\n",
    "print(\"=\"*60)\n",
    "print(\"‚úì Dataset copied and ready for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8c66a5",
   "metadata": {},
   "source": [
    "## 5. Quick Environment Test\n",
    "\n",
    "Test with 2 epochs on 10 samples (~2 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabf0545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run quick test to verify everything works (2-3 minutes)\n",
    "!python tests/test_training_small.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64d4b7c",
   "metadata": {},
   "source": [
    "## 6. Start Full Training\n",
    "\n",
    "**Training Configuration:**\n",
    "- Model: UNet (31M parameters)\n",
    "- Epochs: 50\n",
    "- Batch Size: 16 (optimal for P100 GPU with 16GB memory)\n",
    "- Expected Duration: ~5-7 hours\n",
    "- Metrics: MRE, SDR@2mm, SDR@2.5mm, SDR@3mm, SDR@4mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb183ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training with production settings\n",
    "!python train.py \\\n",
    "    --model unet \\\n",
    "    --epochs 50 \\\n",
    "    --batch-size 16 \\\n",
    "    --experiment-name kaggle_p100_production_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2693c686",
   "metadata": {},
   "source": [
    "## 7. View Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9121914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Find latest experiment\n",
    "experiment_dirs = sorted(glob.glob('experiments/results/unet/*'), key=os.path.getmtime, reverse=True)\n",
    "\n",
    "if experiment_dirs:\n",
    "    latest_exp = experiment_dirs[0]\n",
    "    print(f\"Latest experiment: {latest_exp}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load training history\n",
    "    history_file = os.path.join(latest_exp, 'training_history.json')\n",
    "    if os.path.exists(history_file):\n",
    "        with open(history_file, 'r') as f:\n",
    "            history = json.load(f)\n",
    "        \n",
    "        print(f\"\\nBest Validation Loss: {history['best_val_loss']:.4f}\")\n",
    "        print(f\"Best Validation Metric: {history['best_val_metric']:.4f}\")\n",
    "        \n",
    "        # Show final metrics\n",
    "        if history['val_metrics']:\n",
    "            final_metrics = history['val_metrics'][-1]\n",
    "            print(\"\\nFinal Validation Metrics:\")\n",
    "            print(f\"  MRE: {final_metrics.get('MRE', 'N/A'):.2f} pixels\")\n",
    "            print(f\"  SDR@2mm: {final_metrics.get('SDR_2.0mm', 0)*100:.2f}%\")\n",
    "            print(f\"  SDR@2.5mm: {final_metrics.get('SDR_2.5mm', 0)*100:.2f}%\")\n",
    "            print(f\"  SDR@3mm: {final_metrics.get('SDR_3.0mm', 0)*100:.2f}%\")\n",
    "            print(f\"  SDR@4mm: {final_metrics.get('SDR_4.0mm', 0)*100:.2f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Saved files:\")\n",
    "    !ls -lh {latest_exp}\n",
    "else:\n",
    "    print(\"No experiments found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0d6dbe",
   "metadata": {},
   "source": [
    "## 8. Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf69981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# Find latest experiment\n",
    "experiment_dirs = sorted(glob.glob('experiments/results/unet/*'), key=os.path.getmtime, reverse=True)\n",
    "\n",
    "if experiment_dirs:\n",
    "    latest_exp = experiment_dirs[0]\n",
    "    history_file = os.path.join(latest_exp, 'training_history.json')\n",
    "    \n",
    "    if os.path.exists(history_file):\n",
    "        with open(history_file, 'r') as f:\n",
    "            history = json.load(f)\n",
    "        \n",
    "        # Create figure with subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Plot 1: Loss\n",
    "        axes[0, 0].plot(history['train_losses'], label='Train Loss', marker='o')\n",
    "        axes[0, 0].plot(history['val_losses'], label='Val Loss', marker='s')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].set_title('Training and Validation Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: MRE\n",
    "        mre_values = [m.get('MRE', 0) for m in history['val_metrics']]\n",
    "        axes[0, 1].plot(mre_values, label='Val MRE', marker='o', color='orange')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('MRE (pixels)')\n",
    "        axes[0, 1].set_title('Mean Radial Error')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: SDR metrics\n",
    "        sdr_2mm = [m.get('SDR_2.0mm', 0)*100 for m in history['val_metrics']]\n",
    "        sdr_25mm = [m.get('SDR_2.5mm', 0)*100 for m in history['val_metrics']]\n",
    "        sdr_3mm = [m.get('SDR_3.0mm', 0)*100 for m in history['val_metrics']]\n",
    "        sdr_4mm = [m.get('SDR_4.0mm', 0)*100 for m in history['val_metrics']]\n",
    "        \n",
    "        axes[1, 0].plot(sdr_2mm, label='SDR@2mm', marker='o')\n",
    "        axes[1, 0].plot(sdr_25mm, label='SDR@2.5mm', marker='s')\n",
    "        axes[1, 0].plot(sdr_3mm, label='SDR@3mm', marker='^')\n",
    "        axes[1, 0].plot(sdr_4mm, label='SDR@4mm', marker='d')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('SDR (%)')\n",
    "        axes[1, 0].set_title('Successful Detection Rates')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Summary text\n",
    "        axes[1, 1].axis('off')\n",
    "        summary_text = f\"\"\"\n",
    "        Training Summary\n",
    "        ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        \n",
    "        Experiment: {os.path.basename(latest_exp)}\n",
    "        \n",
    "        Total Epochs: {len(history['train_losses'])}\n",
    "        \n",
    "        Best Val Loss: {history['best_val_loss']:.4f}\n",
    "        \n",
    "        Final Metrics:\n",
    "          ‚Ä¢ MRE: {mre_values[-1]:.2f} pixels\n",
    "          ‚Ä¢ SDR@2mm: {sdr_2mm[-1]:.2f}%\n",
    "          ‚Ä¢ SDR@2.5mm: {sdr_25mm[-1]:.2f}%\n",
    "          ‚Ä¢ SDR@3mm: {sdr_3mm[-1]:.2f}%\n",
    "          ‚Ä¢ SDR@4mm: {sdr_4mm[-1]:.2f}%\n",
    "        \n",
    "        Best Epoch: {history['val_losses'].index(min(history['val_losses'])) + 1}\n",
    "        \"\"\"\n",
    "        axes[1, 1].text(0.1, 0.5, summary_text, fontsize=12, family='monospace',\n",
    "                       verticalalignment='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(latest_exp, 'training_curves.png'), dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\n‚úì Plot saved to: {os.path.join(latest_exp, 'training_curves.png')}\")\n",
    "else:\n",
    "    print(\"No experiments found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18f5c25",
   "metadata": {},
   "source": [
    "## 9. Download Results\n",
    "\n",
    "Download the trained model and results to your local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7c496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Find latest experiment\n",
    "experiment_dirs = sorted(glob.glob('experiments/results/unet/*'), key=os.path.getmtime, reverse=True)\n",
    "\n",
    "if experiment_dirs:\n",
    "    latest_exp = experiment_dirs[0]\n",
    "    exp_name = os.path.basename(latest_exp)\n",
    "    \n",
    "    # Create archive\n",
    "    archive_name = f'{exp_name}_results'\n",
    "    shutil.make_archive(archive_name, 'zip', latest_exp)\n",
    "    \n",
    "    print(f\"‚úì Results archived: {archive_name}.zip\")\n",
    "    print(f\"\\nFile size: {os.path.getsize(archive_name + '.zip') / (1024*1024):.2f} MB\")\n",
    "    print(\"\\nTo download:\")\n",
    "    print(\"  1. Check the 'Output' tab on the right ‚Üí\")\n",
    "    print(f\"  2. Download {archive_name}.zip\")\n",
    "    \n",
    "    # List archive contents\n",
    "    print(\"\\nArchive contains:\")\n",
    "    !unzip -l {archive_name}.zip | head -20\n",
    "else:\n",
    "    print(\"No experiments found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f9cec5",
   "metadata": {},
   "source": [
    "## üìù Notes\n",
    "\n",
    "### Batch Size Guidelines:\n",
    "- **P100 (16GB)**: Use batch size 16 (optimal)\n",
    "- **T4 (15GB)**: Use batch size 8\n",
    "- **K80 (12GB)**: Use batch size 4\n",
    "\n",
    "### To Change Batch Size:\n",
    "```python\n",
    "!python train.py --model unet --epochs 50 --batch-size 8 --experiment-name my_experiment\n",
    "```\n",
    "\n",
    "### To Resume Training:\n",
    "```python\n",
    "!python train.py --model unet --resume experiments/results/unet/your_experiment/checkpoints/last_model.pth\n",
    "```\n",
    "\n",
    "### Expected Training Time:\n",
    "- **P100**: ~5-7 hours (50 epochs, batch size 16)\n",
    "- **T4**: ~8-10 hours (50 epochs, batch size 8)\n",
    "\n",
    "### Target Metrics (After 50 epochs):\n",
    "- MRE: < 30 pixels\n",
    "- SDR@2mm: > 70%\n",
    "- SDR@3mm: > 85%"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
